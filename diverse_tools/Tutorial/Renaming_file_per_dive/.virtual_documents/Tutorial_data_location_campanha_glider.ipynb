


import obspy
from obspy.taup import TauPyModel

from multiprocessing import Pool
from obspy import read,UTCDateTime,Trace
from obspy.clients.fdsn import Client
import os
import glob
import numpy as np
from collections import defaultdict
import pandas as pd

#para plotar as figuras
import matplotlib.pyplot as plt
from matplotlib.transforms import offset_copy
import matplotlib.ticker as ticker
from mpl_toolkits.axes_grid1 import make_axes_locatable
from matplotlib.colors import ListedColormap
import matplotlib.dates as mdates
from mpl_toolkits.axes_grid1.inset_locator import InsetPosition,inset_axes
import matplotlib.colors as colors
import matplotlib.cm as cm
from matplotlib.dates import YearLocator, MonthLocator, DayLocator, HourLocator, MinuteLocator, SecondLocator, DateFormatter
from matplotlib.ticker import MultipleLocator, FormatStrFormatter

from datetime import datetime,timedelta,date
from tqdm import tqdm

from shapely.geometry.polygon import LinearRing

import cartopy.io.shapereader as shpreader
import cartopy.crs as ccrs
import cartopy.feature as cfeature

from IPython.display import HTML
from IPython import display





FOLDER_OUTPUT = '/media/sysop/8d2362fc-3b46-49a7-a864-19b2a6ad097b/diogoloc/dados_posdoc/gliders_project/OUTPUT/'

MSEED_INPUT = "/media/sysop/8d2362fc-3b46-49a7-a864-19b2a6ad097b/diogoloc/dados_posdoc/gliders_project/OUTPUT/MSEED/"

METADATA_OUTPUT = "/media/sysop/8d2362fc-3b46-49a7-a864-19b2a6ad097b/diogoloc/dados_posdoc/gliders_project/OUTPUT/METADATA/"

filename_csv = '/media/sysop/8d2362fc-3b46-49a7-a864-19b2a6ad097b/diogoloc/dados_posdoc/gliders_project/gliders_data/info_csv/metadados_glider_acustico_pmpas-bs.csv'

campanha_csv = '/media/sysop/8d2362fc-3b46-49a7-a864-19b2a6ad097b/diogoloc/dados_posdoc/gliders_project/Equipamentos/informacoes_sensores_gliders/Dados_campanhas_Gliders.xls'





filenames_MSEED = sorted(glob.glob(MSEED_INPUT+'*/*/*.mseed'))


def mseed_data_2_dataframe(i):
    subdir, filename_wav = os.path.split(i)
    root_file = i
    filename = filename_wav.split('.mseed')[0]
    if 'pa' in filename.split('_')[0]:
        mergulho = filename.split('_')[0].split('a')[1]
        stream_number = filename.split('_')[1]

        year_month_day = filename.split('_')[2]
        hour_minute_second = filename.split('_')[3]

        year = int('20'+year_month_day[:2])
        month = int(year_month_day[2:4])
        day = int(year_month_day[4:])

        hour = int(hour_minute_second[:2])
        minute = int(hour_minute_second[2:4])
        second = int(hour_minute_second[4:])

        d = UTCDateTime(datetime(year,month,day,hour,minute,second).isoformat())


    if 'pa' in filename.split('_')[2]:

        mergulho = filename.split('_')[2].split('a')[1]
        stream_number = filename.split('_')[3]

        year_month_day = filename.split('_')[0]
        hour_minute_second = filename.split('_')[1]

        year = int('20'+year_month_day[:2])
        month = int(year_month_day[2:4])
        day = int(year_month_day[4:])

        hour = int(hour_minute_second[:2])
        minute = int(hour_minute_second[2:4])
        second = int(hour_minute_second[4:])

        d = UTCDateTime(datetime(year,month,day,hour,minute,second).isoformat())
        
    
    st = read(i,headonly=True)   
    #----------------------------
    #Starting Dataframe

    starttime = st[0].stats.starttime.datetime
    endtime = st[0].stats.endtime.datetime
    sampling_rate = st[0].stats.sampling_rate
    npts = st[0].stats.npts

    
    df = pd.DataFrame([[root_file],[filename],[mergulho],[stream_number],[starttime],[endtime],[sampling_rate],[npts]], index=['root_file','filename_mseed', 'mergulho', 'stream_number','starttime','endtime','sampling_rate','npts']).T
    
    #Ending Dataframe
    #----------------------------
    return df


pandas_mseed_lst = []

with Pool(processes=8) as p:
    max_ = len(filenames_MSEED)
    with tqdm(total=max_) as pbar:
        for result in p.imap_unordered(mseed_data_2_dataframe,filenames_MSEED):
            pbar.update()
            pandas_mseed_lst.append(result)


dataframe_mseed_final = pd.concat(pandas_mseed_lst, ignore_index=True)


dataframe_mseed_final['starttime'] = pd.to_datetime(dataframe_mseed_final.starttime, format='%Y-%m-%d %H:%M:%S')
dataframe_mseed_final['endtime'] = pd.to_datetime(dataframe_mseed_final.endtime, format='%Y-%m-%d %H:%M:%S')


dataframe_mseed_final['date'] = dataframe_mseed_final['starttime'].dt.date


dataframe_mseed_final





ciclos_dataframe_csv = pd.read_excel(campanha_csv)


ciclos_dataframe_csv


ciclos_dataframe_csv['starttime'] = pd.to_datetime(ciclos_dataframe_csv.starttime, format='%Y-%m-%d %H:%M:%S').dt.date
ciclos_dataframe_csv['endtime'] = pd.to_datetime(ciclos_dataframe_csv.endtime, format='%Y-%m-%d %H:%M:%S').dt.date


ciclos_dataframe_csv['Ciclo_new'] = range(len(ciclos_dataframe_csv))
ciclos_dataframe_csv['Hidrofone'] = ciclos_dataframe_csv['Hidrofone'].astype(str)
ciclos_dataframe_csv['PreAmp'] = ciclos_dataframe_csv['PreAmp'].astype(str)


ciclos_dataframe_csv





dataframe_csv = pd.read_csv(filename_csv,parse_dates=['time'])
dataframe_csv.sort_values(by='time')


df_csv = dataframe_csv.groupby("filename").agg(pd.Series.tolist)


df_csv['filename_mseed'] = df_csv.index.str.replace('_rms_spl3.mat', '')


df_csv.sort_values(by='time')


def name_to_mergulho_stream(filename):
    if 'pa' in filename.split('_')[0]:
        mergulho = filename.split('_')[0].split('a')[1]

    if 'pa' in filename.split('_')[2]:
        mergulho = filename.split('_')[2].split('a')[1]
    
    return mergulho


def calcular_media(lista):
    if len(lista) == 0:
        return None  # Retorna None se a lista estiver vazia para evitar erros
    else:
        return pd.Series(lista).mean()


def flatten(l):
    return l[0]


df_csv['mergulho'] = df_csv['filename_mseed'].apply(name_to_mergulho_stream)


df_csv['date'] = df_csv['time'].apply(calcular_media).dt.date


df_csv


df_csv['lat'] =  df_csv['latitude'].apply(calcular_media)
df_csv['lon'] =  df_csv['longitude'].apply(calcular_media)
df_csv['dep'] =  df_csv['depth'].apply(calcular_media)
df_csv['time'] =  df_csv['time'].apply(flatten)


df_csv_mean = df_csv.drop(columns=['latitude','longitude','depth',])


df_csv_mean.sort_values(by='date')


df_csv_meanS = df_csv_mean.sort_values(by='time')


len(sorted(df_csv_mean['filename_mseed'].to_list())) - len(sorted(dataframe_mseed_final['filename_mseed'].to_list()))





ciclos_dataframe_csv


def nearest(items, pivot):
    return min(items, key=lambda x: abs(x - pivot))


def convert_36(line):
    '''
    Creating the station name according to the hexatrigesimal two-digit algoritm conversion
    '''
    cycle_lst = [i for i in range(500)]
    
    # hexatrigesimal two digit number for each cycle:
    hexatri_lst = [str(np.base_repr(y, 36)).zfill(2) for y in cycle_lst]
    
    #find the nearest date and append the 2-hexatrigesimal-digit:
    return 'G'+line['starttime'].strftime('%y')+hexatri_lst[cycle_lst.index(nearest(cycle_lst,line['Ciclo_new']))]


ciclos_dataframe_csvS = ciclos_dataframe_csv.sort_values(by='starttime')


ciclos_dataframe_csvS['station_nettab'] = ciclos_dataframe_csvS.apply(convert_36,axis=1)


ciclos_dataframe_csvS


def nettab_name_csv(line):
    '''
    Getting the station name code from other dataframe
    '''
    
    filtro = (line.date >= ciclos_dataframe_csvS["starttime"]) & (line.date <= ciclos_dataframe_csvS["endtime"])
    
    df2 = ciclos_dataframe_csvS[filtro]

    if len(df2) > 0:
        return df2['station_nettab'].values[0],df2['N/S'].values[0],df2['Fs'].values[0],df2['nBits'].values[0],df2['Hidrofone'].values[0],df2['PreAmp'].values[0],df2['Ciclo_new'].values[0]


df_csv_meanS_rows = [row for idx,row in df_csv_meanS.iterrows()]


DF_PACK = []
with Pool(processes=4) as p:
    max_ = len(df_csv_meanS_rows)
    with tqdm(total=max_) as pbar:
        for result in p.imap_unordered(nettab_name_csv,df_csv_meanS_rows):
            pbar.update()
            if result is not None:
                DF_PACK.append(result)


df_csv_meanS['station_nettab'] = [i[0] for i in DF_PACK]
df_csv_meanS['N/S'] = [i[1] for i in DF_PACK]
df_csv_meanS['Fs'] = [i[2] for i in DF_PACK]
df_csv_meanS['nBits'] = [i[3] for i in DF_PACK]
df_csv_meanS['Hidrofone'] = [i[4] for i in DF_PACK]
df_csv_meanS['PreAmp'] = [i[5] for i in DF_PACK]
df_csv_meanS['Ciclo'] = [i[6] for i in DF_PACK]


df_csv_meanS


df_campanha = df_csv_meanS.groupby(["station_nettab"]).agg(pd.Series.tolist)


df_campanha.columns


plt.figure()
for i in df_campanha.iterrows():
    plt.plot(i[1]['N/S'][0],i[1].time[0],'ok')


def mergulho_start_end_time(k):
    try:
        files_mseed_HD = [i for i in filenames_MSEED if i.split('/')[-1].split('.')[0] in k[1]['filename_mseed']]
        
        tr_start = obspy.read(files_mseed_HD[0],headonly=True)[0]

        tr_end = obspy.read(files_mseed_HD[-1],headonly=True)[0]
        
        filename_mseed_HD = [i.split('/')[-1].split('.')[0] for i in files_mseed_HD]
        df_file = k[1].to_frame()

        df_nettab = df_file.T
        df_nettab['station_nettab'] = k[0]
        df_nettab['date_start_nettab'] = tr_start.stats.starttime.strftime('%Y/%j:%H%M')
        df_nettab['date_end_nettab'] = tr_end.stats.endtime.strftime('%Y/%j:%H%M')
        df_nettab['filename_mseed_HD'] = [filename_mseed_HD]
        df_nettab['start_lat'] = k[1]['lat'][0]
        df_nettab['end_lat'] = k[1]['lat'][-1]
        df_nettab['start_lon'] = k[1]['lon'][0]
        df_nettab['end_lon'] = k[1]['lon'][-1]
        df_nettab['ciclo'] = k[1]['Ciclo'][0]
        df_nettab['Fs'] = k[1]['Fs'][0]
        df_nettab['nBits'] = k[1]['nBits'][0]        

        #Modelo do Glider
        df_nettab['N/S'] = [k[1]['N/S']]
        df_nettab['modelo'] = k[1]['N/S'][0] 


        #Modelo do Sensor
        df_nettab['hidrofone'] = [k[1]['Hidrofone']]
        if '9' in k[1]['Hidrofone'][0]:
            df_nettab['sensor'] = 'HTI_92_WB'
        if 'D' in k[1]['Hidrofone'][0]:
            df_nettab['sensor'] = 'M36-V35-100'

        #Modelo do Registrador
        df_nettab['preAmp'] = [k[1]['PreAmp']]
        if '1' in k[1]['PreAmp'][0]:
            df_nettab['registrador'] = 'EOS_HM1'
        else:
            df_nettab['registrador'] = k[1]['PreAmp'][0]

        return df_nettab
    
    except:
        pass


end_time = []
with Pool(processes=4) as p:
    with tqdm(total=len(df_campanha)) as pbar:
        for result in p.imap(mergulho_start_end_time,df_campanha.iterrows()):
            pbar.update()
            end_time.append(result)


dataframe_campanha_mseed_final = pd.concat(end_time, ignore_index=True)


dataframe_campanha_mseed_final.head(4).sort_values(by='ciclo')


lines_nettab = []
for i in dataframe_campanha_mseed_final.iterrows():
    for k,l in enumerate(tqdm(i[1]["filename_mseed"],desc='Creating Nettab file')):
        try:
            df_filtrada = dataframe_mseed_final[dataframe_mseed_final["filename_mseed"].str.contains(i[1]["filename_mseed"][k])]
            
            # Station lines (one line per station/sensor/epoch) # code description datalogger%sn seismometer%sn # sampling orientation lat. lon. elev. depth. start end
            #Sl: UNAP "Uni-Iquique/Chile"   DM24%A1383 CMG-3ESP/60%T34622 100 ZNE -20.24393 -70.14041 0.0    0.0 2009/134 
            lines_nettab.append('Sl: '+str(i[1]['station_nettab'])+' "C'+str(i[1]['ciclo']).zfill(2)+':'+i[1]['modelo']+'" '+i[1]['registrador']+' '+str(i[1]['sensor'])+' 100 H(0.0,0.0) '+str(i[1]['lat'][k])[:8]+' '+str(i[1]['lon'][k])[:8]+' 0 0 '+UTCDateTime(str(df_filtrada['starttime'].values[0])).strftime('%Y/%j:%H%M')+' '+UTCDateTime(str(df_filtrada['endtime'].values[0])).strftime('%Y/%j:%H%M'))
        except:
            print(df_filtrada)


lines_nettab = []
for i in dataframe_campanha_mseed_final.iterrows():
    # Station lines (one line per station/sensor/epoch) # code description datalogger%sn seismometer%sn # sampling orientation lat. lon. elev. depth. start end
    #Sl: UNAP "Uni-Iquique/Chile"   DM24%A1383 CMG-3ESP/60%T34622 100 ZNE -20.24393 -70.14041 0.0    0.0 2009/134 
    lines_nettab.append('Sl: '+str(i[1]['station_nettab'])+' "C'+str(i[1]['ciclo']).zfill(2)+':'+i[1]['modelo']+'" '+i[1]['registrador']+' '+str(i[1]['sensor'])+' 100 H '+str(i[1]['start_lat'])[:8]+' '+str(i[1]['start_lon'])[:8]+' 0 0 '+i[1]['date_start_nettab']+' '+i[1]['date_end_nettab'])
    print('Sl: '+str(i[1]['station_nettab'])+' "C'+str(i[1]['ciclo']).zfill(2)+':'+i[1]['modelo']+'" '+i[1]['registrador']+' '+i[1]['sensor']+' 100 H(0.0,0.0) '+str(i[1]['start_lat'])[:8]+' '+str(i[1]['start_lon'])[:8]+' 0 0 '+i[1]['date_start_nettab']+' '+i[1]['date_end_nettab'])





len(lines_nettab)


with open('/media/sysop/8d2362fc-3b46-49a7-a864-19b2a6ad097b/diogoloc/dados_posdoc/gliders_project/OUTPUT/NETTAB/glider_nettab_full.txt', 'w') as f:
    for line in lines_nettab:
        f.write(line)
        f.write('\n')


dataframe_campanha_mseed_final.to_feather('/media/sysop/8d2362fc-3b46-49a7-a864-19b2a6ad097b/diogoloc/dados_posdoc/gliders_project/OUTPUT/NETTAB/df_campanha_glider.feather')





dataframe_campanha_mseed_final_rows = [row for idx,row in dataframe_campanha_mseed_final.iterrows()]


rename_files_lst = []
with Pool(processes=4) as p:
    with tqdm(total=len(df_campanha)) as pbar:
        for result in p.imap(get_rename_files,dataframe_campanha_mseed_final_rows):
            pbar.update()
            rename_files_lst.append(result)


def rename_files(row):
    
    files_mseed_HD = row['filename_mseed_HD']
    
    files_path_mseed_HD = [glob.glob(MSEED_INPUT+'*/*/'+i+'.mseed') for i in files_mseed_HD]
    
    for y in files_path_mseed_HD:
        
        st = obspy.read(y[0])
        st[0].stats.network = 'GL'
        st[0].stats.station = k['station_nettab']
        st[0].stats.channel = 'HHH'

        #<SDSdir>/Year/NET/STA/CHAN.TYPE/NET.STA.LOC.CHAN.TYPE.YEAR.DAY
        #Saving in SDS structure        
        OUTPUT_STREAM = FOLDER_OUTPUT+'MSEED_GL_SDS/'+st[0].stats.starttime.strftime("%Y")+'/'+st[0].stats.network+'/'+st[0].stats.station+'/HHH.D/'
        os.makedirs(OUTPUT_STREAM,exist_ok=True)
        
        station_name_file = st[0].stats.network+'.'+st[0].stats.station+'..'+st[0].stats.channel+'.D.'+st[0].stats.starttime.strftime("%Y.%j.%H.%M.%S")+'.mseed'
        if os.path.exists(OUTPUT_STREAM+station_name_file) == False:
            st.write(OUTPUT_STREAM+station_name_file, format='MSEED')


with Pool(processes=4) as p:
    with tqdm(total=len(dataframe_campanha_mseed_final)) as pbar:
        for result in p.imap_unordered(rename_files,dataframe_campanha_mseed_final_rows):
            pbar.update()



