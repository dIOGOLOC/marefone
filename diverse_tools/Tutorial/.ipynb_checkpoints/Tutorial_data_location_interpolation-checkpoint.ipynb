{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a82d8275",
   "metadata": {},
   "source": [
    "# Importando módulos "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "36e98d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import obspy\n",
    "from obspy.taup import TauPyModel\n",
    "\n",
    "from multiprocessing import Pool\n",
    "from obspy import read,UTCDateTime,Trace\n",
    "from obspy.clients.fdsn import Client\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "#para plotar as figuras\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.transforms import offset_copy\n",
    "import matplotlib.ticker as ticker\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from matplotlib.colors import ListedColormap\n",
    "import matplotlib.dates as mdates\n",
    "from mpl_toolkits.axes_grid1.inset_locator import InsetPosition,inset_axes\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.dates import YearLocator, MonthLocator, DayLocator, HourLocator, MinuteLocator, SecondLocator, DateFormatter\n",
    "from matplotlib.ticker import MultipleLocator, FormatStrFormatter\n",
    "\n",
    "from datetime import datetime,timedelta,date\n",
    "from tqdm import tqdm\n",
    "\n",
    "from shapely.geometry.polygon import LinearRing\n",
    "\n",
    "import cartopy.io.shapereader as shpreader\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4f9f8f",
   "metadata": {},
   "source": [
    "# Inputs e Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38736b57",
   "metadata": {},
   "outputs": [],
   "source": [
    "FOLDER_OUTPUT = '/home/diogoloc/dados_posdoc/glider_ON_petrobras/OUTPUT/'\n",
    "\n",
    "EVENT_FOLDER = '/home/diogoloc/dados_posdoc/glider_ON_petrobras/events_selected/'\n",
    "\n",
    "MSEED_INPUT = \"/run/media/dIOGOLOC/8d2362fc-3b46-49a7-a864-19b2a6ad097b/diogoloc/dados_posdoc/gliders_project/OUTPUT/MSEED/\"\n",
    "\n",
    "METADATA_OUTPUT = \"/home/diogoloc/dados_posdoc/glider_ON_petrobras/metadata/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebce2fe",
   "metadata": {},
   "source": [
    "# Extraindo informações dos arquivos \".mseed\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "95a67b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_MSEED = sorted(glob.glob(MSEED_INPUT+'*/*/*.mseed'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d2378588",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mseed_data_2_dataframe(i):\n",
    "    subdir, filename_wav = os.path.split(i)\n",
    "    filename = filename_wav.split('.mseed')[0]\n",
    "    if 'pa' in filename.split('_')[0]:\n",
    "        mergulho = filename.split('_')[0].split('a')[1]\n",
    "        stream_number = filename.split('_')[1]\n",
    "\n",
    "        year_month_day = filename.split('_')[2]\n",
    "        hour_minute_second = filename.split('_')[3]\n",
    "\n",
    "        year = int('20'+year_month_day[:2])\n",
    "        month = int(year_month_day[2:4])\n",
    "        day = int(year_month_day[4:])\n",
    "\n",
    "        hour = int(hour_minute_second[:2])\n",
    "        minute = int(hour_minute_second[2:4])\n",
    "        second = int(hour_minute_second[4:])\n",
    "\n",
    "        d = UTCDateTime(datetime(year,month,day,hour,minute,second).isoformat())\n",
    "\n",
    "\n",
    "    if 'pa' in filename.split('_')[2]:\n",
    "\n",
    "        mergulho = filename.split('_')[2].split('a')[1]\n",
    "        stream_number = filename.split('_')[3]\n",
    "\n",
    "        year_month_day = filename.split('_')[0]\n",
    "        hour_minute_second = filename.split('_')[1]\n",
    "\n",
    "        year = int('20'+year_month_day[:2])\n",
    "        month = int(year_month_day[2:4])\n",
    "        day = int(year_month_day[4:])\n",
    "\n",
    "        hour = int(hour_minute_second[:2])\n",
    "        minute = int(hour_minute_second[2:4])\n",
    "        second = int(hour_minute_second[4:])\n",
    "\n",
    "        d = UTCDateTime(datetime(year,month,day,hour,minute,second).isoformat())\n",
    "        \n",
    "    \n",
    "    st = read(i,headonly=True)   \n",
    "    #----------------------------\n",
    "    #Starting Dataframe\n",
    "\n",
    "    starttime = st[0].stats.starttime.datetime\n",
    "    endtime = st[0].stats.endtime.datetime\n",
    "    sampling_rate = st[0].stats.sampling_rate\n",
    "    npts = st[0].stats.npts\n",
    "\n",
    "    \n",
    "    df = pd.DataFrame([[filename],[mergulho],[stream_number],[starttime],[endtime],[sampling_rate],[npts]], index=['filename_mseed', 'mergulho', 'stream_number','starttime','endtime','sampling_rate','npts']).T\n",
    "    \n",
    "    #Ending Dataframe\n",
    "    #----------------------------\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de4c7652",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "pandas_mseed_lst = []\n",
    "\n",
    "with Pool(processes=8) as p:\n",
    "    max_ = len(filenames_MSEED)\n",
    "    with tqdm(total=max_) as pbar:\n",
    "        for result in p.imap_unordered(mseed_data_2_dataframe,filenames_MSEED):\n",
    "            pbar.update()\n",
    "            pandas_mseed_lst.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d1380549",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataframe_mseed_final \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconcat\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpandas_mseed_lst\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/programs/anaconda3/lib/python3.10/site-packages/pandas/util/_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[1;32m    330\u001b[0m     )\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/programs/anaconda3/lib/python3.10/site-packages/pandas/core/reshape/concat.py:368\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobjs\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconcat\u001b[39m(\n\u001b[1;32m    148\u001b[0m     objs: Iterable[NDFrame] \u001b[38;5;241m|\u001b[39m Mapping[HashableT, NDFrame],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    157\u001b[0m     copy: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    158\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m    159\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;124;03m    Concatenate pandas objects along a particular axis.\u001b[39;00m\n\u001b[1;32m    161\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    366\u001b[0m \u001b[38;5;124;03m    1   3   4\u001b[39;00m\n\u001b[1;32m    367\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 368\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_Concatenator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobjs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    372\u001b[0m \u001b[43m        \u001b[49m\u001b[43mjoin\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    373\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkeys\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    374\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    375\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnames\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnames\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    376\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverify_integrity\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverify_integrity\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    377\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    378\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/programs/anaconda3/lib/python3.10/site-packages/pandas/core/reshape/concat.py:425\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    422\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    424\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 425\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    428\u001b[0m     objs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "dataframe_mseed_final = pd.concat(pandas_mseed_lst, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a6ca54",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_mseed_final.sort_values(by='starttime')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bccb0c9",
   "metadata": {},
   "source": [
    "# Extraindo informações dos arquivos \".csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0377fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_csv = '/home/diogoloc/dados_posdoc/glider_ON_petrobras/info_csv/metadados_glider_acustico_pmpas-bs.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa226236",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframe_csv = pd.read_csv(filename_csv,parse_dates=['time'])\n",
    "dataframe_csv.sort_values(by='time')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "063c1023",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv = dataframe_csv.groupby(\"filename\").agg(pd.Series.tolist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c74ebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0a8103f9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m df_csv[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfilename_mseed\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf_csv\u001b[49m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_rms_spl3.mat\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_csv' is not defined"
     ]
    }
   ],
   "source": [
    "df_csv['filename_mseed'] = df_csv.index.str.replace('_rms_spl3.mat', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3fe0a6d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df_csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf_csv\u001b[49m\u001b[38;5;241m.\u001b[39msort_values(by\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df_csv' is not defined"
     ]
    }
   ],
   "source": [
    "df_csv.sort_values(by='time')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b3be7c",
   "metadata": {},
   "source": [
    "# Aglutinando os dataframes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c0f9f19",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_csv_mseed = df_csv.merge(dataframe_mseed_final, on='filename_mseed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "088fbdd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>depth</th>\n",
       "      <th>filename_mseed</th>\n",
       "      <th>mergulho</th>\n",
       "      <th>stream_number</th>\n",
       "      <th>starttime</th>\n",
       "      <th>endtime</th>\n",
       "      <th>sampling_rate</th>\n",
       "      <th>npts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[2019-02-10 14:09:33.999998, 2019-02-10 14:10:...</td>\n",
       "      <td>[-24.446863, -24.44698, -24.4471, -24.447107, ...</td>\n",
       "      <td>[-42.37369, -42.373936, -42.37419, -42.374313,...</td>\n",
       "      <td>[1.100106, 1.3757317, 4.2127805, 11.835875, 17...</td>\n",
       "      <td>pa0001au_001_190210_140934</td>\n",
       "      <td>0001</td>\n",
       "      <td>001</td>\n",
       "      <td>2019-02-10 14:09:34</td>\n",
       "      <td>2019-02-10 14:19:33.990</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2019-02-10 14:19:34.000002, 2019-02-10 14:20:...</td>\n",
       "      <td>[-24.446705, -24.446661, -24.44662, -24.446585...</td>\n",
       "      <td>[-42.376213, -42.376427, -42.376637, -42.37684...</td>\n",
       "      <td>[53.095825, 58.468796, 63.91838, 69.02441, 73....</td>\n",
       "      <td>pa0001au_002_190210_141934</td>\n",
       "      <td>0001</td>\n",
       "      <td>002</td>\n",
       "      <td>2019-02-10 14:19:34</td>\n",
       "      <td>2019-02-10 14:29:33.990</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2019-02-10 14:29:33.999997, 2019-02-10 14:30:...</td>\n",
       "      <td>[-24.44634, -24.446304, -24.446268, -24.446236...</td>\n",
       "      <td>[-42.378624, -42.37893, -42.3792, -42.379448, ...</td>\n",
       "      <td>[100.04637, 104.443794, 109.028496, 113.73068,...</td>\n",
       "      <td>pa0001au_003_190210_142934</td>\n",
       "      <td>0001</td>\n",
       "      <td>003</td>\n",
       "      <td>2019-02-10 14:29:34</td>\n",
       "      <td>2019-02-10 14:39:33.990</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[2019-02-10 14:39:34.000001, 2019-02-10 14:40:...</td>\n",
       "      <td>[-24.446028, -24.44601, -24.445984, -24.445984...</td>\n",
       "      <td>[-42.38127, -42.381603, -42.381893, -42.38208,...</td>\n",
       "      <td>[147.0188, 151.68568, 156.39667, 160.77971, 16...</td>\n",
       "      <td>pa0001au_004_190210_143934</td>\n",
       "      <td>0001</td>\n",
       "      <td>004</td>\n",
       "      <td>2019-02-10 14:39:34</td>\n",
       "      <td>2019-02-10 14:49:33.990</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[2019-02-10 14:49:33.999996, 2019-02-10 14:50:...</td>\n",
       "      <td>[-24.445845, -24.445818, -24.445793, -24.44576...</td>\n",
       "      <td>[-42.383965, -42.38423, -42.384487, -42.384743...</td>\n",
       "      <td>[190.9082, 195.09427, 199.34274, 203.43884, 20...</td>\n",
       "      <td>pa0001au_005_190210_144934</td>\n",
       "      <td>0001</td>\n",
       "      <td>005</td>\n",
       "      <td>2019-02-10 14:49:34</td>\n",
       "      <td>2019-02-10 14:59:33.990</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6902</th>\n",
       "      <td>[2019-10-10 21:10:37.000005, 2019-10-10 21:11:...</td>\n",
       "      <td>[-24.700645, -24.700645, -24.700645, -24.70064...</td>\n",
       "      <td>[-41.329514, -41.329514, -41.329514, -41.32951...</td>\n",
       "      <td>[810.5255, 810.5255, 810.5255, 810.5255, 810.5...</td>\n",
       "      <td>pa0238au_013_191010_211037</td>\n",
       "      <td>0238</td>\n",
       "      <td>013</td>\n",
       "      <td>2019-10-10 21:10:37</td>\n",
       "      <td>2019-10-10 21:20:36.990</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6903</th>\n",
       "      <td>[2019-10-10 21:20:36.999999, 2019-10-10 21:21:...</td>\n",
       "      <td>[-24.700645, -24.700645, -24.700645, -24.70064...</td>\n",
       "      <td>[-41.329514, -41.329514, -41.329514, -41.32951...</td>\n",
       "      <td>[810.5255, 810.5255, 810.5255, 810.5255, 810.5...</td>\n",
       "      <td>pa0238au_014_191010_212037</td>\n",
       "      <td>0238</td>\n",
       "      <td>014</td>\n",
       "      <td>2019-10-10 21:20:37</td>\n",
       "      <td>2019-10-10 21:30:36.990</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6904</th>\n",
       "      <td>[2019-10-10 21:30:37.000004, 2019-10-10 21:31:...</td>\n",
       "      <td>[-24.700645, -24.700645, -24.700645, -24.70064...</td>\n",
       "      <td>[-41.329514, -41.329514, -41.329514, -41.32951...</td>\n",
       "      <td>[810.5255, 810.5255, 810.5255, 810.5255, 810.5...</td>\n",
       "      <td>pa0238au_015_191010_213037</td>\n",
       "      <td>0238</td>\n",
       "      <td>015</td>\n",
       "      <td>2019-10-10 21:30:37</td>\n",
       "      <td>2019-10-10 21:40:36.990</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6905</th>\n",
       "      <td>[2019-10-10 21:40:36.999998, 2019-10-10 21:41:...</td>\n",
       "      <td>[-24.700645, -24.700645, -24.700645, -24.70064...</td>\n",
       "      <td>[-41.329514, -41.329514, -41.329514, -41.32951...</td>\n",
       "      <td>[810.5255, 810.5255, 810.5255, 810.5255, 810.5...</td>\n",
       "      <td>pa0238au_016_191010_214037</td>\n",
       "      <td>0238</td>\n",
       "      <td>016</td>\n",
       "      <td>2019-10-10 21:40:37</td>\n",
       "      <td>2019-10-10 21:50:36.990</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6906</th>\n",
       "      <td>[2019-10-10 21:50:37.000003, 2019-10-10 21:51:...</td>\n",
       "      <td>[-24.700645, -24.700645, -24.700645, -24.70064...</td>\n",
       "      <td>[-41.329514, -41.329514, -41.329514, -41.32951...</td>\n",
       "      <td>[810.5255, 810.5255, 810.5255, 810.5255, 810.5...</td>\n",
       "      <td>pa0238au_017_191010_215037</td>\n",
       "      <td>0238</td>\n",
       "      <td>017</td>\n",
       "      <td>2019-10-10 21:50:37</td>\n",
       "      <td>2019-10-10 21:59:05.130</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50814</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8852 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   time  \\\n",
       "0     [2019-02-10 14:09:33.999998, 2019-02-10 14:10:...   \n",
       "1     [2019-02-10 14:19:34.000002, 2019-02-10 14:20:...   \n",
       "2     [2019-02-10 14:29:33.999997, 2019-02-10 14:30:...   \n",
       "3     [2019-02-10 14:39:34.000001, 2019-02-10 14:40:...   \n",
       "4     [2019-02-10 14:49:33.999996, 2019-02-10 14:50:...   \n",
       "...                                                 ...   \n",
       "6902  [2019-10-10 21:10:37.000005, 2019-10-10 21:11:...   \n",
       "6903  [2019-10-10 21:20:36.999999, 2019-10-10 21:21:...   \n",
       "6904  [2019-10-10 21:30:37.000004, 2019-10-10 21:31:...   \n",
       "6905  [2019-10-10 21:40:36.999998, 2019-10-10 21:41:...   \n",
       "6906  [2019-10-10 21:50:37.000003, 2019-10-10 21:51:...   \n",
       "\n",
       "                                               latitude  \\\n",
       "0     [-24.446863, -24.44698, -24.4471, -24.447107, ...   \n",
       "1     [-24.446705, -24.446661, -24.44662, -24.446585...   \n",
       "2     [-24.44634, -24.446304, -24.446268, -24.446236...   \n",
       "3     [-24.446028, -24.44601, -24.445984, -24.445984...   \n",
       "4     [-24.445845, -24.445818, -24.445793, -24.44576...   \n",
       "...                                                 ...   \n",
       "6902  [-24.700645, -24.700645, -24.700645, -24.70064...   \n",
       "6903  [-24.700645, -24.700645, -24.700645, -24.70064...   \n",
       "6904  [-24.700645, -24.700645, -24.700645, -24.70064...   \n",
       "6905  [-24.700645, -24.700645, -24.700645, -24.70064...   \n",
       "6906  [-24.700645, -24.700645, -24.700645, -24.70064...   \n",
       "\n",
       "                                              longitude  \\\n",
       "0     [-42.37369, -42.373936, -42.37419, -42.374313,...   \n",
       "1     [-42.376213, -42.376427, -42.376637, -42.37684...   \n",
       "2     [-42.378624, -42.37893, -42.3792, -42.379448, ...   \n",
       "3     [-42.38127, -42.381603, -42.381893, -42.38208,...   \n",
       "4     [-42.383965, -42.38423, -42.384487, -42.384743...   \n",
       "...                                                 ...   \n",
       "6902  [-41.329514, -41.329514, -41.329514, -41.32951...   \n",
       "6903  [-41.329514, -41.329514, -41.329514, -41.32951...   \n",
       "6904  [-41.329514, -41.329514, -41.329514, -41.32951...   \n",
       "6905  [-41.329514, -41.329514, -41.329514, -41.32951...   \n",
       "6906  [-41.329514, -41.329514, -41.329514, -41.32951...   \n",
       "\n",
       "                                                  depth  \\\n",
       "0     [1.100106, 1.3757317, 4.2127805, 11.835875, 17...   \n",
       "1     [53.095825, 58.468796, 63.91838, 69.02441, 73....   \n",
       "2     [100.04637, 104.443794, 109.028496, 113.73068,...   \n",
       "3     [147.0188, 151.68568, 156.39667, 160.77971, 16...   \n",
       "4     [190.9082, 195.09427, 199.34274, 203.43884, 20...   \n",
       "...                                                 ...   \n",
       "6902  [810.5255, 810.5255, 810.5255, 810.5255, 810.5...   \n",
       "6903  [810.5255, 810.5255, 810.5255, 810.5255, 810.5...   \n",
       "6904  [810.5255, 810.5255, 810.5255, 810.5255, 810.5...   \n",
       "6905  [810.5255, 810.5255, 810.5255, 810.5255, 810.5...   \n",
       "6906  [810.5255, 810.5255, 810.5255, 810.5255, 810.5...   \n",
       "\n",
       "                  filename_mseed mergulho stream_number           starttime  \\\n",
       "0     pa0001au_001_190210_140934     0001           001 2019-02-10 14:09:34   \n",
       "1     pa0001au_002_190210_141934     0001           002 2019-02-10 14:19:34   \n",
       "2     pa0001au_003_190210_142934     0001           003 2019-02-10 14:29:34   \n",
       "3     pa0001au_004_190210_143934     0001           004 2019-02-10 14:39:34   \n",
       "4     pa0001au_005_190210_144934     0001           005 2019-02-10 14:49:34   \n",
       "...                          ...      ...           ...                 ...   \n",
       "6902  pa0238au_013_191010_211037     0238           013 2019-10-10 21:10:37   \n",
       "6903  pa0238au_014_191010_212037     0238           014 2019-10-10 21:20:37   \n",
       "6904  pa0238au_015_191010_213037     0238           015 2019-10-10 21:30:37   \n",
       "6905  pa0238au_016_191010_214037     0238           016 2019-10-10 21:40:37   \n",
       "6906  pa0238au_017_191010_215037     0238           017 2019-10-10 21:50:37   \n",
       "\n",
       "                     endtime sampling_rate   npts  \n",
       "0    2019-02-10 14:19:33.990         100.0  60000  \n",
       "1    2019-02-10 14:29:33.990         100.0  60000  \n",
       "2    2019-02-10 14:39:33.990         100.0  60000  \n",
       "3    2019-02-10 14:49:33.990         100.0  60000  \n",
       "4    2019-02-10 14:59:33.990         100.0  60000  \n",
       "...                      ...           ...    ...  \n",
       "6902 2019-10-10 21:20:36.990         100.0  60000  \n",
       "6903 2019-10-10 21:30:36.990         100.0  60000  \n",
       "6904 2019-10-10 21:40:36.990         100.0  60000  \n",
       "6905 2019-10-10 21:50:36.990         100.0  60000  \n",
       "6906 2019-10-10 21:59:05.130         100.0  50814  \n",
       "\n",
       "[8852 rows x 11 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df_csv_mseed.sort_values(by='starttime')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d9df5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir uma função para converter a lista de tempos em timestamps\n",
    "def converter_para_timestamp(tempos):\n",
    "    return [pd.to_datetime(tempo).timestamp() for tempo in tempos]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "063c8cb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df_csv_mseed['time_timestamp'] = merged_df_csv_mseed['time'].apply(lambda x: converter_para_timestamp(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e4de033b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>depth</th>\n",
       "      <th>filename_mseed</th>\n",
       "      <th>mergulho</th>\n",
       "      <th>stream_number</th>\n",
       "      <th>starttime</th>\n",
       "      <th>endtime</th>\n",
       "      <th>sampling_rate</th>\n",
       "      <th>npts</th>\n",
       "      <th>time_timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[2019-02-10 14:09:33.999998, 2019-02-10 14:10:...</td>\n",
       "      <td>[-24.446863, -24.44698, -24.4471, -24.447107, ...</td>\n",
       "      <td>[-42.37369, -42.373936, -42.37419, -42.374313,...</td>\n",
       "      <td>[1.100106, 1.3757317, 4.2127805, 11.835875, 17...</td>\n",
       "      <td>pa0001au_001_190210_140934</td>\n",
       "      <td>0001</td>\n",
       "      <td>001</td>\n",
       "      <td>2019-02-10 14:09:34</td>\n",
       "      <td>2019-02-10 14:19:33.990</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60000</td>\n",
       "      <td>[1549807773.999998, 1549807834.000004, 1549807...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[2019-02-10 14:19:34.000002, 2019-02-10 14:20:...</td>\n",
       "      <td>[-24.446705, -24.446661, -24.44662, -24.446585...</td>\n",
       "      <td>[-42.376213, -42.376427, -42.376637, -42.37684...</td>\n",
       "      <td>[53.095825, 58.468796, 63.91838, 69.02441, 73....</td>\n",
       "      <td>pa0001au_002_190210_141934</td>\n",
       "      <td>0001</td>\n",
       "      <td>002</td>\n",
       "      <td>2019-02-10 14:19:34</td>\n",
       "      <td>2019-02-10 14:29:33.990</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60000</td>\n",
       "      <td>[1549808374.000002, 1549808433.999999, 1549808...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[2019-02-10 14:29:33.999997, 2019-02-10 14:30:...</td>\n",
       "      <td>[-24.44634, -24.446304, -24.446268, -24.446236...</td>\n",
       "      <td>[-42.378624, -42.37893, -42.3792, -42.379448, ...</td>\n",
       "      <td>[100.04637, 104.443794, 109.028496, 113.73068,...</td>\n",
       "      <td>pa0001au_003_190210_142934</td>\n",
       "      <td>0001</td>\n",
       "      <td>003</td>\n",
       "      <td>2019-02-10 14:29:34</td>\n",
       "      <td>2019-02-10 14:39:33.990</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60000</td>\n",
       "      <td>[1549808973.999997, 1549809034.000003, 1549809...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[2019-02-10 14:39:34.000001, 2019-02-10 14:40:...</td>\n",
       "      <td>[-24.446028, -24.44601, -24.445984, -24.445984...</td>\n",
       "      <td>[-42.38127, -42.381603, -42.381893, -42.38208,...</td>\n",
       "      <td>[147.0188, 151.68568, 156.39667, 160.77971, 16...</td>\n",
       "      <td>pa0001au_004_190210_143934</td>\n",
       "      <td>0001</td>\n",
       "      <td>004</td>\n",
       "      <td>2019-02-10 14:39:34</td>\n",
       "      <td>2019-02-10 14:49:33.990</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60000</td>\n",
       "      <td>[1549809574.000001, 1549809633.999998, 1549809...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[2019-02-10 14:49:33.999996, 2019-02-10 14:50:...</td>\n",
       "      <td>[-24.445845, -24.445818, -24.445793, -24.44576...</td>\n",
       "      <td>[-42.383965, -42.38423, -42.384487, -42.384743...</td>\n",
       "      <td>[190.9082, 195.09427, 199.34274, 203.43884, 20...</td>\n",
       "      <td>pa0001au_005_190210_144934</td>\n",
       "      <td>0001</td>\n",
       "      <td>005</td>\n",
       "      <td>2019-02-10 14:49:34</td>\n",
       "      <td>2019-02-10 14:59:33.990</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60000</td>\n",
       "      <td>[1549810173.999996, 1549810234.000002, 1549810...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8847</th>\n",
       "      <td>[2019-05-01 07:42:51.999997, 2019-05-01 07:43:...</td>\n",
       "      <td>[-26.480818, -26.480764, -26.480757, -26.48075...</td>\n",
       "      <td>[-45.451607, -45.451553, -45.451447, -45.45134...</td>\n",
       "      <td>[432.7796, 442.0454, 451.03748, 459.97116, 468...</td>\n",
       "      <td>pa0489au_006_190501_074252</td>\n",
       "      <td>0489</td>\n",
       "      <td>006</td>\n",
       "      <td>2019-05-01 07:42:52</td>\n",
       "      <td>2019-05-01 07:52:51.990</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60000</td>\n",
       "      <td>[1556696571.999997, 1556696632.000003, 1556696...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8848</th>\n",
       "      <td>[2019-05-01 07:52:52.000001, 2019-05-01 07:53:...</td>\n",
       "      <td>[-26.480371, -26.480392, -26.480354, -26.4803,...</td>\n",
       "      <td>[-45.451046, -45.45092, -45.450855, -45.450806...</td>\n",
       "      <td>[522.99854, 531.8356, 540.42163, 549.1145, 557...</td>\n",
       "      <td>pa0489au_007_190501_075252</td>\n",
       "      <td>0489</td>\n",
       "      <td>007</td>\n",
       "      <td>2019-05-01 07:52:52</td>\n",
       "      <td>2019-05-01 08:02:51.990</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60000</td>\n",
       "      <td>[1556697172.000001, 1556697231.999998, 1556697...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8849</th>\n",
       "      <td>[2019-05-01 08:02:51.999996, 2019-05-01 08:03:...</td>\n",
       "      <td>[-26.480185, -26.480198, -26.480215, -26.48016...</td>\n",
       "      <td>[-45.450188, -45.45007, -45.449947, -45.4499, ...</td>\n",
       "      <td>[608.7978, 617.0209, 625.4136, 634.215, 642.91...</td>\n",
       "      <td>pa0489au_008_190501_080252</td>\n",
       "      <td>0489</td>\n",
       "      <td>008</td>\n",
       "      <td>2019-05-01 08:02:52</td>\n",
       "      <td>2019-05-01 08:12:51.990</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60000</td>\n",
       "      <td>[1556697771.999996, 1556697832.000002, 1556697...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8850</th>\n",
       "      <td>[2019-05-01 08:12:52, 2019-05-01 08:13:51.9999...</td>\n",
       "      <td>[-26.479984, -26.47998, -26.479994, -26.480034...</td>\n",
       "      <td>[-45.44935, -45.44924, -45.449127, -45.44899, ...</td>\n",
       "      <td>[695.72845, 704.39856, 713.2655, 721.90985, 73...</td>\n",
       "      <td>pa0489au_009_190501_081252</td>\n",
       "      <td>0489</td>\n",
       "      <td>009</td>\n",
       "      <td>2019-05-01 08:12:52</td>\n",
       "      <td>2019-05-01 08:22:51.990</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60000</td>\n",
       "      <td>[1556698372.0, 1556698431.999997, 1556698492.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8851</th>\n",
       "      <td>[2019-05-01 08:22:52.000005, 2019-05-01 08:23:...</td>\n",
       "      <td>[-26.479904, -26.479887, -26.479876, -26.47988...</td>\n",
       "      <td>[-45.448395, -45.44831, -45.448215, -45.448105...</td>\n",
       "      <td>[783.3346, 792.0588, 800.68896, 809.35254, 818...</td>\n",
       "      <td>pa0489au_010_190501_082252</td>\n",
       "      <td>0489</td>\n",
       "      <td>010</td>\n",
       "      <td>2019-05-01 08:22:52</td>\n",
       "      <td>2019-05-01 08:32:51.990</td>\n",
       "      <td>100.0</td>\n",
       "      <td>60000</td>\n",
       "      <td>[1556698972.000005, 1556699032.000001, 1556699...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8852 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   time  \\\n",
       "0     [2019-02-10 14:09:33.999998, 2019-02-10 14:10:...   \n",
       "1     [2019-02-10 14:19:34.000002, 2019-02-10 14:20:...   \n",
       "2     [2019-02-10 14:29:33.999997, 2019-02-10 14:30:...   \n",
       "3     [2019-02-10 14:39:34.000001, 2019-02-10 14:40:...   \n",
       "4     [2019-02-10 14:49:33.999996, 2019-02-10 14:50:...   \n",
       "...                                                 ...   \n",
       "8847  [2019-05-01 07:42:51.999997, 2019-05-01 07:43:...   \n",
       "8848  [2019-05-01 07:52:52.000001, 2019-05-01 07:53:...   \n",
       "8849  [2019-05-01 08:02:51.999996, 2019-05-01 08:03:...   \n",
       "8850  [2019-05-01 08:12:52, 2019-05-01 08:13:51.9999...   \n",
       "8851  [2019-05-01 08:22:52.000005, 2019-05-01 08:23:...   \n",
       "\n",
       "                                               latitude  \\\n",
       "0     [-24.446863, -24.44698, -24.4471, -24.447107, ...   \n",
       "1     [-24.446705, -24.446661, -24.44662, -24.446585...   \n",
       "2     [-24.44634, -24.446304, -24.446268, -24.446236...   \n",
       "3     [-24.446028, -24.44601, -24.445984, -24.445984...   \n",
       "4     [-24.445845, -24.445818, -24.445793, -24.44576...   \n",
       "...                                                 ...   \n",
       "8847  [-26.480818, -26.480764, -26.480757, -26.48075...   \n",
       "8848  [-26.480371, -26.480392, -26.480354, -26.4803,...   \n",
       "8849  [-26.480185, -26.480198, -26.480215, -26.48016...   \n",
       "8850  [-26.479984, -26.47998, -26.479994, -26.480034...   \n",
       "8851  [-26.479904, -26.479887, -26.479876, -26.47988...   \n",
       "\n",
       "                                              longitude  \\\n",
       "0     [-42.37369, -42.373936, -42.37419, -42.374313,...   \n",
       "1     [-42.376213, -42.376427, -42.376637, -42.37684...   \n",
       "2     [-42.378624, -42.37893, -42.3792, -42.379448, ...   \n",
       "3     [-42.38127, -42.381603, -42.381893, -42.38208,...   \n",
       "4     [-42.383965, -42.38423, -42.384487, -42.384743...   \n",
       "...                                                 ...   \n",
       "8847  [-45.451607, -45.451553, -45.451447, -45.45134...   \n",
       "8848  [-45.451046, -45.45092, -45.450855, -45.450806...   \n",
       "8849  [-45.450188, -45.45007, -45.449947, -45.4499, ...   \n",
       "8850  [-45.44935, -45.44924, -45.449127, -45.44899, ...   \n",
       "8851  [-45.448395, -45.44831, -45.448215, -45.448105...   \n",
       "\n",
       "                                                  depth  \\\n",
       "0     [1.100106, 1.3757317, 4.2127805, 11.835875, 17...   \n",
       "1     [53.095825, 58.468796, 63.91838, 69.02441, 73....   \n",
       "2     [100.04637, 104.443794, 109.028496, 113.73068,...   \n",
       "3     [147.0188, 151.68568, 156.39667, 160.77971, 16...   \n",
       "4     [190.9082, 195.09427, 199.34274, 203.43884, 20...   \n",
       "...                                                 ...   \n",
       "8847  [432.7796, 442.0454, 451.03748, 459.97116, 468...   \n",
       "8848  [522.99854, 531.8356, 540.42163, 549.1145, 557...   \n",
       "8849  [608.7978, 617.0209, 625.4136, 634.215, 642.91...   \n",
       "8850  [695.72845, 704.39856, 713.2655, 721.90985, 73...   \n",
       "8851  [783.3346, 792.0588, 800.68896, 809.35254, 818...   \n",
       "\n",
       "                  filename_mseed mergulho stream_number           starttime  \\\n",
       "0     pa0001au_001_190210_140934     0001           001 2019-02-10 14:09:34   \n",
       "1     pa0001au_002_190210_141934     0001           002 2019-02-10 14:19:34   \n",
       "2     pa0001au_003_190210_142934     0001           003 2019-02-10 14:29:34   \n",
       "3     pa0001au_004_190210_143934     0001           004 2019-02-10 14:39:34   \n",
       "4     pa0001au_005_190210_144934     0001           005 2019-02-10 14:49:34   \n",
       "...                          ...      ...           ...                 ...   \n",
       "8847  pa0489au_006_190501_074252     0489           006 2019-05-01 07:42:52   \n",
       "8848  pa0489au_007_190501_075252     0489           007 2019-05-01 07:52:52   \n",
       "8849  pa0489au_008_190501_080252     0489           008 2019-05-01 08:02:52   \n",
       "8850  pa0489au_009_190501_081252     0489           009 2019-05-01 08:12:52   \n",
       "8851  pa0489au_010_190501_082252     0489           010 2019-05-01 08:22:52   \n",
       "\n",
       "                     endtime sampling_rate   npts  \\\n",
       "0    2019-02-10 14:19:33.990         100.0  60000   \n",
       "1    2019-02-10 14:29:33.990         100.0  60000   \n",
       "2    2019-02-10 14:39:33.990         100.0  60000   \n",
       "3    2019-02-10 14:49:33.990         100.0  60000   \n",
       "4    2019-02-10 14:59:33.990         100.0  60000   \n",
       "...                      ...           ...    ...   \n",
       "8847 2019-05-01 07:52:51.990         100.0  60000   \n",
       "8848 2019-05-01 08:02:51.990         100.0  60000   \n",
       "8849 2019-05-01 08:12:51.990         100.0  60000   \n",
       "8850 2019-05-01 08:22:51.990         100.0  60000   \n",
       "8851 2019-05-01 08:32:51.990         100.0  60000   \n",
       "\n",
       "                                         time_timestamp  \n",
       "0     [1549807773.999998, 1549807834.000004, 1549807...  \n",
       "1     [1549808374.000002, 1549808433.999999, 1549808...  \n",
       "2     [1549808973.999997, 1549809034.000003, 1549809...  \n",
       "3     [1549809574.000001, 1549809633.999998, 1549809...  \n",
       "4     [1549810173.999996, 1549810234.000002, 1549810...  \n",
       "...                                                 ...  \n",
       "8847  [1556696571.999997, 1556696632.000003, 1556696...  \n",
       "8848  [1556697172.000001, 1556697231.999998, 1556697...  \n",
       "8849  [1556697771.999996, 1556697832.000002, 1556697...  \n",
       "8850  [1556698372.0, 1556698431.999997, 1556698492.0...  \n",
       "8851  [1556698972.000005, 1556699032.000001, 1556699...  \n",
       "\n",
       "[8852 rows x 12 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df_csv_mseed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6c56c206",
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp_data(df,nfile):\n",
    "    \n",
    "    df = df[df['filename_mseed'] == nfile]\n",
    "    mseed_nfile = df['filename_mseed'].values[0]\n",
    "    name_file = sorted(glob.glob(MSEED_INPUT+'*/*/'+mseed_nfile+'*'))[0]\n",
    "    \n",
    "    st = read(name_file,headonly=True)  \n",
    "    starttime = st[0].stats.starttime.timestamp\n",
    "    endtime = st[0].stats.endtime.timestamp\n",
    "    sampling_rate = st[0].stats.sampling_rate\n",
    "    npts = st[0].stats.npts\n",
    "    times = np.array([i.timestamp for i in st[0].times('utcdatetime')])\n",
    "    times_datetime = np.array([i.datetime for i in st[0].times('utcdatetime')])\n",
    "\n",
    "    # Dados de profundidade e tempo\n",
    "    profundidade_lst = df['depth'].values[0]\n",
    "    latitude_lst = df['latitude'].values[0]\n",
    "    longitude_lst = df['longitude'].values[0]\n",
    "    tempo_numerico_lst = df['time_timestamp'].values[0]\n",
    "    tempo_interpolado_numerico = times\n",
    "    \n",
    "    # Interpolar os dados de profundidade usando numpy.interp\n",
    "    profundidade_interpolada = np.interp(tempo_interpolado_numerico, tempo_numerico_lst, profundidade_lst)\n",
    "\n",
    "    # Interpolar os dados de longitude usando numpy.interp\n",
    "    longitude_interpolada = np.interp(tempo_interpolado_numerico, tempo_numerico_lst, longitude_lst)\n",
    "\n",
    "    # Interpolar os dados de latitude usando numpy.interp\n",
    "    latitude_interpolada = np.interp(tempo_interpolado_numerico, tempo_numerico_lst, latitude_lst)\n",
    "\n",
    "    # Exibir os resultados\n",
    "    df_interpolado = pd.DataFrame({'Datafile':nfile,'Tempo': times_datetime, 'Profundidade_interp': profundidade_interpolada, 'Longitude_interp': longitude_interpolada, 'Latitude_interp': latitude_interpolada})\n",
    "\n",
    "    #Criando pasta para salvar os metadados \n",
    "    folder_feather_name = METADATA_OUTPUT+st[0].stats.starttime.strftime('%Y')+'/'+st[0].stats.starttime.strftime('%Y-%m-%d')+'/'\n",
    "    os.makedirs(folder_feather_name,exist_ok=True)\n",
    "    df_interpolado.to_feather(folder_feather_name+mseed_nfile+'.feather')\n",
    "    \n",
    "    return df_interpolado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59b39b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_interpol(namfile):\n",
    "    df_interpol = interp_data(df=merged_df_csv_mseed,nfile=namfile)\n",
    "    \n",
    "    return df_interpol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda06436",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██████████▏                            | 2318/8852 [05:29<13:48,  7.89it/s]"
     ]
    }
   ],
   "source": [
    "pandas_mseed_lst = []\n",
    "\n",
    "with Pool(processes=16) as p:\n",
    "    max_ = len(merged_df_csv_mseed['filename_mseed'].values)\n",
    "    with tqdm(total=max_) as pbar:\n",
    "        for result in p.imap_unordered(start_interpol,merged_df_csv_mseed['filename_mseed'].values):\n",
    "            pbar.update()\n",
    "            pandas_mseed_lst.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9729040d",
   "metadata": {},
   "source": [
    "# Importando metadados:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffe8eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_metadata = sorted(glob.glob(METADATA_OUTPUT+'*/*/*.feather'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c9634e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for nfile_meta in tqdm(filenames_metadata,total=len(filenames_metadata)):\n",
    "\n",
    "    #########################################################################################################################################################\n",
    "    #STREAM\n",
    "    file_n_meta = nfile_meta.split('/')[-1].split('.feather')[0]\n",
    "    \n",
    "    name_file = sorted(glob.glob(MSEED_INPUT+'*/*/'+file_n_meta+'*'))[0]\n",
    "    \n",
    "    st = read(name_file,headonly=False)  \n",
    "    starttime = st[0].stats.starttime\n",
    "    endtime = st[0].stats.endtime\n",
    "    sampling_rate = st[0].stats.sampling_rate\n",
    "    npts = st[0].stats.npts\n",
    "    \n",
    "    df_meta = pd.read_feather(nfile_meta)\n",
    "\n",
    "    lat_mean_info = df_meta['Latitude_interp'].mean()\n",
    "    lon_mean_info = df_meta['Longitude_interp'].mean()       \n",
    "    depth_mean_info = df_meta['Profundidade_interp'].mean()\n",
    "\n",
    "    lat_std_info = df_meta['Latitude_interp'].std()\n",
    "    lon_std_info = df_meta['Longitude_interp'].std()       \n",
    "    depth_std_info = df_meta['Profundidade_interp'].std()\n",
    "    \n",
    "    \n",
    "    if 'pa' in df_meta['Datafile'][0].split('_')[0]:\n",
    "        mergulho = df_meta['Datafile'][0].split('_')[0].split('a')[1]\n",
    "        stream_number = df_meta['Datafile'][0].split('_')[1]\n",
    "\n",
    "    if 'pa' in df_meta['Datafile'][0].split('_')[2]:\n",
    "        mergulho = df_meta['Datafile'][0].split('_')[2].split('a')[1]\n",
    "        stream_number = df_meta['Datafile'][0].split('_')[3]\n",
    "                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a642f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "for nfile_meta in tqdm(filenames_metadata,total=len(filenames_metadata)):\n",
    "\n",
    "    #########################################################################################################################################################\n",
    "    #STREAM\n",
    "    file_n_meta = nfile_meta.split('/')[-1].split('.feather')[0]\n",
    "    \n",
    "    name_file = sorted(glob.glob(MSEED_INPUT+'*/*/'+file_n_meta+'*'))[0]\n",
    "    \n",
    "    st = read(name_file,headonly=False)  \n",
    "    starttime = st[0].stats.starttime\n",
    "    endtime = st[0].stats.endtime\n",
    "    sampling_rate = st[0].stats.sampling_rate\n",
    "    npts = st[0].stats.npts\n",
    "    \n",
    "    df_meta = pd.read_feather(nfile_meta)\n",
    "\n",
    "    lat_mean_info = df_meta['Latitude_interp'].mean()\n",
    "    lon_mean_info = df_meta['Longitude_interp'].mean()       \n",
    "    depth_mean_info = df_meta['Profundidade_interp'].mean()\n",
    "\n",
    "    lat_std_info = df_meta['Latitude_interp'].std()\n",
    "    lon_std_info = df_meta['Longitude_interp'].std()       \n",
    "    depth_std_info = df_meta['Profundidade_interp'].std()\n",
    "    \n",
    "    \n",
    "    if 'pa' in df_meta['Datafile'][0].split('_')[0]:\n",
    "        mergulho = df_meta['Datafile'][0].split('_')[0].split('a')[1]\n",
    "        stream_number = df_meta['Datafile'][0].split('_')[1]\n",
    "\n",
    "    if 'pa' in df_meta['Datafile'][0].split('_')[2]:\n",
    "        mergulho = df_meta['Datafile'][0].split('_')[2].split('a')[1]\n",
    "        stream_number = df_meta['Datafile'][0].split('_')[3]\n",
    "                        \n",
    "    #########################################################################################################################################################\n",
    "    #Figure \n",
    "\n",
    "    # set up the plot and create a GeoAxes:\n",
    "    proj = ccrs.PlateCarree()\n",
    "\n",
    "    fig, (ax,ax1) = plt.subplots(2,1,figsize=(16,16))\n",
    "    ax = plt.subplot(2, 1, 1, projection=proj)\n",
    "                     \n",
    "    # ----------------------------------------------------------------------------------------------------------\n",
    "    # Limit the extent of the map to a small longitude/latitude range.\n",
    "    latmin=-27\n",
    "    latmax=-22\n",
    "    lonmin=-50\n",
    "    lonmax=-40\n",
    "\n",
    "    ax.set_extent([lonmin,lonmax, latmin, latmax], crs=ccrs.Geodetic())\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------------\n",
    "    # Ploting lat/lon values\n",
    "                \n",
    "    h = ax.scatter(df_meta['Longitude_interp'].values,df_meta['Latitude_interp'].values,c=np.array([mdates.date2num(i) for i in df_meta['Tempo'].dt.to_pydatetime()]),marker='o',alpha=0.8,cmap='plasma',s=75,transform=proj)\n",
    "    ax.text(0.85, 0.05,'Lat:'+str(round(lat_mean_info,2))+r'$\\pm$'+str(round(lat_std_info,4)), horizontalalignment='left',verticalalignment='center', transform=ax.transAxes)\n",
    "    ax.text(0.85, 0.1,'Lon:'+str(round(lon_mean_info,2))+r'$\\pm$'+str(round(lon_std_info,4)), horizontalalignment='left',verticalalignment='center', transform=ax.transAxes)\n",
    "    ax.text(0.85, 0.15,'Prof:'+str(round(depth_mean_info,2))+r'$\\pm$'+str(round(depth_std_info,4)), horizontalalignment='left',verticalalignment='center', transform=ax.transAxes)\n",
    "    # ----------------------------------------------------------------------------------------------------------\n",
    "    # Adding background map \n",
    "    ax.add_feature(cfeature.LAND)\n",
    "    ax.add_feature(cfeature.OCEAN)\n",
    "    ax.add_feature(cfeature.COASTLINE,linewidth=0.3)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':',linewidth=0.3)\n",
    "    ax.gridlines(crs=ccrs.PlateCarree(), draw_labels=True,linewidth=1, color='gray', alpha=0.5, linestyle='--')\n",
    "    ax.set_title('Dia: '+starttime.strftime('%d-%m-%Y'))\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------------------------------\n",
    "    # Adding colorbar\n",
    "    divider = make_axes_locatable(ax)\n",
    "    ax_cb = divider.new_horizontal(size=\"1%\", pad=0.6, axes_class=plt.Axes)\n",
    "\n",
    "    fig.add_axes(ax_cb)\n",
    "    cb = plt.colorbar(h, cax=ax_cb)\n",
    "    cb.ax.yaxis.set_major_formatter(mdates.DateFormatter('%H:%M:%S'))\n",
    "\n",
    "    #####################################################\n",
    "    # Adding inset axes RIGHT\n",
    "    #####################################################\n",
    "\n",
    "    axins = plt.axes([0.2, 0.55, 0.2, 0.2],projection=proj)\n",
    "\n",
    "    axins.scatter(df_meta['Longitude_interp'].values,df_meta['Latitude_interp'].values,c=np.array([mdates.date2num(i) for i in df_meta['Tempo'].dt.to_pydatetime()]),marker='o',alpha=0.3,cmap='plasma',s=75,transform=proj)\n",
    "\n",
    "    axins.add_feature(cfeature.LAND)\n",
    "    axins.add_feature(cfeature.OCEAN)\n",
    "    axins.add_feature(cfeature.COASTLINE,linewidth=0.3)\n",
    "    axins.add_feature(cfeature.BORDERS, linestyle=':',linewidth=0.3)\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------------------------------\n",
    "    # Adding grid \n",
    "    axins.gridlines(crs=ccrs.PlateCarree(), draw_labels=[\"bottom\", \"left\", \"right\"],linewidth=0.5, color='gray', alpha=0.5, linestyle='--')\n",
    "    axins.tick_params(axis='both',which=\"both\",bottom=True, top=True, left=True, right=True,labelbottom=True, labeltop=False, labelleft=True, labelright=True)\n",
    "\n",
    "    # subregion of the original image\n",
    "    x1 = np.min(lon_mean_info)-abs(np.min(lon_mean_info)/1000)\n",
    "    x2 = np.max(lon_mean_info)+abs(np.max(lon_mean_info)/1000)\n",
    "    y1 = np.min(lat_mean_info)-abs(np.min(lat_mean_info)/1000)\n",
    "    y2 = np.max(lat_mean_info)+abs(np.max(lat_mean_info)/1000)\n",
    "    axins.set_xlim(x1, x2)\n",
    "    axins.set_ylim(y1, y2)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------------\n",
    "    # Adding ZOOM\n",
    "    ax.indicate_inset_zoom(axins, edgecolor=\"black\")\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------------\n",
    "    ax_histx = plt.axes([0.2, 0.73, 0.2, 0.1],sharex=axins,facecolor='lightsteelblue')\n",
    "    ax_histx.scatter(df_meta['Longitude_interp'].values,df_meta['Profundidade_interp'].values*-1,c=np.array([mdates.date2num(i) for i in df_meta['Tempo'].dt.to_pydatetime()]),marker='o',alpha=0.7,cmap='plasma',s=100)\n",
    "    ax_histx.grid(linewidth=0.5, color='gray', alpha=0.5, linestyle='--')\n",
    "    ax_histx.set_title(\"Mergulho:\"+mergulho+\"\\n stream:\"+stream_number)\n",
    "    ax_histx.tick_params(axis='both',which=\"both\",bottom=True, top=True, left=True, right=True,labelbottom=False, labeltop=False, labelleft=True, labelright=True)\n",
    "    \n",
    "    depth_lim_max = (np.min(df_meta['Profundidade_interp'])-abs(np.min(df_meta['Profundidade_interp'])/3000))*-1\n",
    "    depth_lim_min = (np.max(df_meta['Profundidade_interp'])+abs(np.max(df_meta['Profundidade_interp'])/3000))*-1\n",
    "    ax_histx.set_ylim(depth_lim_min,depth_lim_max)\n",
    "    \n",
    "    \n",
    "    ax_histx.yaxis.set_major_formatter('{x} m')\n",
    "    ax_histx.yaxis.set_major_locator(MultipleLocator(10))\n",
    "    ax_histx.yaxis.set_minor_locator(MultipleLocator(1))\n",
    "    \n",
    "    # ----------------------------------------------------------------------------------------------------------\n",
    "    # Adding global location map\n",
    "    # inset location relative to main plot (ax) in normalized units\n",
    "    inset_x = 0\n",
    "    inset_y = 1\n",
    "    inset_size = 0.5\n",
    "\n",
    "    # Adding Geoaxes\n",
    "    ax2 = plt.axes([0, 0, 1, 1], projection=ccrs.Orthographic(central_latitude=(latmin + latmax)/2,central_longitude=(lonmin + lonmax) / 2))\n",
    "    ax2.set_global()\n",
    "\n",
    "    # Adding background map \n",
    "    ax2.add_feature(cfeature.LAND)\n",
    "    ax2.add_feature(cfeature.OCEAN)\n",
    "    ax2.add_feature(cfeature.COASTLINE)\n",
    "\n",
    "    # Adding inset geoaxes position\n",
    "    ip = InsetPosition(ax, [inset_x - inset_size / 2,\n",
    "                          inset_y - inset_size / 2,\n",
    "                          inset_size,inset_size])\n",
    "    \n",
    "    ax2.set_axes_locator(ip)\n",
    "\n",
    "\n",
    "    # Adding red rectangle position\n",
    "    nvert = 100\n",
    "    lons = np.r_[np.linspace(lonmin, lonmin, nvert),\n",
    "                                 np.linspace(lonmin, lonmax, nvert),\n",
    "                                 np.linspace(lonmax, lonmax, nvert)].tolist()\n",
    "    \n",
    "    lats = np.r_[np.linspace(latmin, latmax, nvert),\n",
    "                                 np.linspace(latmax, latmax, nvert),\n",
    "                                 np.linspace(latmax, latmin, nvert)].tolist()\n",
    "\n",
    "    ring = LinearRing(list(zip(lons, lats)))\n",
    "    ax2.add_geometries([ring], ccrs.PlateCarree(),facecolor='none', edgecolor='red', linewidth=0.75)\n",
    "\n",
    "    # ----------------------------------------------------------------------------------------------------------\n",
    "    # Adding stream plot\n",
    "\n",
    "    ax1.plot(st[0].times('matplotlib'),st[0].data,'-k')        \n",
    "    ax1.set_ylabel('Amplitude [counts]')\n",
    "    \n",
    "    locator = mdates.AutoDateLocator(minticks=9, maxticks=14)\n",
    "    formatter = mdates.ConciseDateFormatter(locator)\n",
    "    \n",
    "    ax1.xaxis.set_major_locator(locator)\n",
    "    ax1.xaxis.set_major_formatter(formatter)\n",
    "    \n",
    "    mins2 = SecondLocator(interval=60)\n",
    "    mins1 = SecondLocator(interval=10)\n",
    "    \n",
    "    ax1.xaxis.set_major_locator(mins2)\n",
    "    ax1.xaxis.set_minor_locator(mins1)\n",
    "    ax1.set_title('Arquivo: '+file_n_meta+'.wav')\n",
    "        \n",
    "    # ----------------------------------------------------------------------------------------------------------\n",
    "    # Saving figure\n",
    "    os.makedirs(FOLDER_OUTPUT+'FIGURAS/INTERP_MSEED/'+st[0].stats.starttime.strftime('%Y')+'/'+st[0].stats.starttime.strftime('%Y-%m-%d')+'/',exist_ok=True)\n",
    "    fig.savefig(FOLDER_OUTPUT+'FIGURAS/INTERP_MSEED/'+st[0].stats.starttime.strftime('%Y')+'/'+st[0].stats.starttime.strftime('%Y-%m-%d')+'/'+'interp_mseed_'+file_n_meta+'.png')\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087ed6f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
